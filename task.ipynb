{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install numpy==1.21.4\n",
    "!pip install opencv-python\n",
    "!pip install dlib\n",
    "!pip install matplotlib\n",
    "!pip install Pillow --upgrade\n",
    "!pip install librosa\n",
    "!pip install numba==0.53\n",
    "!pip install opensmile\n",
    "!pip install PyYAML==5.4.1\n",
    "!pip install sox\n",
    "!pip install sklearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/shemetov@ad.speechpro.com/.local/lib/python3.8/site-packages/pkg_resources/__init__.py:122: PkgResourcesDeprecationWarning: 0.1.36ubuntu1 is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n",
      "/home/shemetov@ad.speechpro.com/.local/lib/python3.8/site-packages/pkg_resources/__init__.py:122: PkgResourcesDeprecationWarning: 0.23ubuntu1 is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n",
      "Requirement already satisfied: opencv-contrib-python in /home/shemetov@ad.speechpro.com/.local/lib/python3.8/site-packages (4.6.0.66)\n",
      "Requirement already satisfied: numpy>=1.14.5; python_version >= \"3.7\" in /home/shemetov@ad.speechpro.com/.local/lib/python3.8/site-packages (from opencv-contrib-python) (1.21.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-contrib-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import threading\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "import torchaudio.transforms as T\n",
    "import time\n",
    "from IPython.display import display, Image\n",
    "import ipywidgets as widgets\n",
    "import threading\n",
    "import concurrent.futures\n",
    "from PIL import Image\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import librosa\n",
    "import sklearn\n",
    "import torch\n",
    "import torchaudio\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import librosa.display\n",
    "import librosa.filters\n",
    "import IPython.display as ipd\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import opensmile\n",
    "from sklearn.model_selection import train_test_split\n",
    "from platform import python_version\n",
    "torch.manual_seed(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_of_classes = ['SAD','NEU','HAP','FEA','DIS','ANG']\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "#static_analyze\n",
    "dct_classes = {'SAD':0,'NEU':0,'HAP':0,'FEA':0,'DIS':0,'ANG':0}\n",
    "fig1, ax1 = plt.subplots()\n",
    "\n",
    "for file_path in set_of_classes:\n",
    "    list_of_files = sorted(glob.glob('./CREMA-D/VideoFlash/*' + file_path + '*.flv'))\n",
    "    dct_classes[file_path] = len(list_of_files)\n",
    "ax1.bar(dct_classes.keys(), dct_classes.values())\n",
    "ax1.set_title(\"Count video in each class emotions\")\n",
    "\n",
    "fig2, ax2 = plt.subplots()\n",
    "\n",
    "for file_path in set_of_classes:\n",
    "    list_of_files = sorted(glob.glob('./CREMA-D/AudioClass/' + file_path + '/*.wav'))\n",
    "    dct_classes[file_path] = len(list_of_files)\n",
    "ax2.bar(dct_classes.keys(), dct_classes.values())\n",
    "ax2.set_title(\"Count audio in each class emotions\")\n",
    "\n",
    "\n",
    "fig3, ax3 = plt.subplots()\n",
    "dct_avg_len_of_videos = {'SAD':0,'NEU':0,'HAP':0,'FEA':0,'DIS':0,'ANG':0}\n",
    "for file_path in set_of_classes:\n",
    "    list_of_files = sorted(glob.glob('./CREMA-D/VideoFlash/*' + file_path + '*.flv'))\n",
    "    sum_var = 0\n",
    "    for video_path in list_of_files:\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)      # OpenCV2 version 2 used \"CV_CAP_PROP_FPS\"\n",
    "        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        duration = frame_count/fps\n",
    "        seconds = duration%60\n",
    "        sum_var += seconds\n",
    "    dct_avg_len_of_videos[file_path] = round(sum_var / len(list_of_files),3)\n",
    "\n",
    "ax3.bar(dct_avg_len_of_videos.keys(), dct_avg_len_of_videos.values())\n",
    "ax3.set_title(\"Average duration video in each class emotions\")\n",
    "\n",
    "\n",
    "\n",
    "cap.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cut video on images and save in folders\n",
    "for file_path in set_of_classes:\n",
    "    list_of_files = sorted(glob.glob('./CREMA-D/VideoFlash/*' + file_path + '*.flv'))\n",
    "    count = 0\n",
    "    for video_path in list_of_files:\n",
    "        order_var = re.findall(r'\\d{1,5}',video_path)[0]\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        success,image = cap.read()\n",
    "        while success:\n",
    "            #print(\"CREMA-D/ImageClass/\" + file_path + \"/\" + order_var + \"_frame%d.jpg\" % count)\n",
    "            cv2.imwrite(\"CREMA-D/ImageClass/\" + file_path + \"/\" + order_var + \"_frame%d.jpg\" % count, image)\n",
    "            success,image = cap.read()\n",
    "            count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Face Recognation with HAAD Cascade\n",
    "for file_path in set_of_classes:\n",
    "    list_of_files = sorted(glob.glob('./CREMA-D/ImageClass/' + file_path + '/*.jpg'))\n",
    "    count = 0\n",
    "    for image_path in list_of_files:\n",
    "        order_var = re.findall(r'\\d{1,5}',image_path)[0]\n",
    "        image = cv2.imread(image_path)\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        faceCascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "        face = faceCascade.detectMultiScale(\n",
    "                gray,\n",
    "                scaleFactor=1.2,\n",
    "                minNeighbors=6,\n",
    "                minSize=(30, 30)\n",
    "        )\n",
    "\n",
    "        if len(face) != 0:\n",
    "            x, y, w, h = face[0]\n",
    "        gray_color = gray[y:y + h, x:x + w]\n",
    "        res = cv2.resize(gray_color,(128,128))\n",
    "        #gray = cv2.cvtColor(roi_color, cv2.COLOR_BGR2GRAY)\n",
    "        cv2.imwrite(\"CREMA-D/FaceClass/\" + file_path + \"/\" + order_var + \"_frame%d.jpg\" % count, res)\n",
    "        count += 1\n",
    "        #display(Image.fromarray(res))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SIFT признаки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Dataset with SIFT features for pytorch \n",
    "class ImageDatasetSIFT(torch.utils.data.Dataset):\n",
    "    def __init__(self,dir_paths = None, labels=None, le = None):\n",
    "        super(ImageDatasetSIFT, self).__init__()\n",
    "        self.dir_paths = dir_paths\n",
    "        self.labels = labels\n",
    "        self.image_files = []\n",
    "        self.set_of_labels = []\n",
    "        if self.labels is not None:\n",
    "            for label in self.labels:\n",
    "                images_path = glob.glob(self.dir_paths + label + '/*.jpg')[0:10000]\n",
    "                for image_path in tqdm(images_path):\n",
    "                    gray_image = cv2.imread(image_path,cv2.IMREAD_GRAYSCALE)\n",
    "                    sift = cv2.SIFT_create(20)\n",
    "                    _,des = sift.detectAndCompute(gray_image,None)\n",
    "                    \n",
    "                    if (des is None) or len(des) !=20:\n",
    "                        continue\n",
    "                    else:\n",
    "                        self.image_files.append(des)\n",
    "                len_image_files = len(self.image_files)\n",
    "                label = list(le.transform([label]))\n",
    "                self.set_of_labels += label * len_image_files\n",
    "                #self.image_files += images_path\n",
    "                \n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        label = self.set_of_labels[index]\n",
    "        des = self.image_files[index]\n",
    "        feature = F.normalize(torch.FloatTensor(np.array([des[0:20]])),p=2.0)\n",
    "        return feature, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:24<00:00, 409.82it/s]\n",
      "100%|██████████| 10000/10000 [00:24<00:00, 411.05it/s]\n",
      "100%|██████████| 10000/10000 [00:24<00:00, 410.98it/s]\n",
      "100%|██████████| 10000/10000 [00:24<00:00, 410.18it/s]\n",
      "100%|██████████| 10000/10000 [00:24<00:00, 410.77it/s]\n",
      "100%|██████████| 10000/10000 [00:24<00:00, 409.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49612\n",
      "49611\n",
      "torch.Size([1, 20, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Create traind and test.\n",
    "extractor = None\n",
    "le = sklearn.preprocessing.LabelEncoder()\n",
    "le = le.fit(set_of_classes)\n",
    "dir_path = '/home/shemetov@ad.speechpro.com/Desktop/testpy/CREMA-D/FaceClass/'\n",
    "\n",
    "data_sift = ImageDatasetSIFT(dir_paths=dir_path,labels=set_of_classes,le=le)\n",
    "print(len(data_sift))\n",
    "print(int(len(data_sift) * 0.8) + int(len(data_sift) * 0.2))\n",
    "print(np.shape(next(iter(data_sift))[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sift, test_sift = torch.utils.data.random_split(data_sift,[int(len(data_sift) * 0.8),int(len(data_sift) * 0.2)+1])\n",
    "train_set_sift = torch.utils.data.DataLoader(train_sift, batch_size=32, shuffle=True)\n",
    "test_set_sift = torch.utils.data.DataLoader(test_sift, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNSIFT(nn.Module):\n",
    "    def __init__(self, in_channels = 1, out_channels = 128, kernel_size = 5):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dp1 = nn.Dropout2d()\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels * 2, 5)\n",
    "        #self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels * 2)\n",
    "        self.dp2 = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(14848, 64)\n",
    "        self.fc2 = nn.Linear(64, 40)\n",
    "        self.fc3 = nn.Linear(40, 6)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu((self.conv1(x))))\n",
    "        #x = self.dp1(x)\n",
    "        x = self.pool(F.relu((self.conv2(x))))\n",
    "        #x = self.dp2(x)\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNNSIFT().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Structure of Model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, in_channels = 1,out_channels = 128,kernel_size = 3):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.pool1 = nn.MaxPool2d(2,2)\n",
    "        self.drop1 = nn.Dropout(0.2)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3)\n",
    "        self.pool2 = nn.MaxPool2d(2,2)\n",
    "        self.drop2 = nn.Dropout(0.2)\n",
    "        self.conv3 = nn.Conv2d(out_channels, 2 * out_channels, 3)\n",
    "        self.bn3 = nn.BatchNorm2d(2 * out_channels)\n",
    "        self.drop3 = nn.Dropout(0.2)\n",
    "        self.pool3 = nn.MaxPool2d(2,2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(11520, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 6)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.pool1(self.conv1(x)))\n",
    "        x = F.relu(self.pool2(self.conv2(x)))\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.drop1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.drop2(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        return F.log_softmax(x,dim=1)  \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "#Optimization for model\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "video_features_lbp = []\n",
    "video_pred_lbp = []\n",
    "#Train model\n",
    "for epoch in range(25):\n",
    "    running_loss = 0.0\n",
    "    for i, data in tqdm(enumerate(train_set_sift, 0)):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999: \n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 1000:.3f}')\n",
    "            running_loss = 0.0\n",
    "    y_pred = None\n",
    "    y_test = None\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs,target in tqdm(test_set_sift):\n",
    "            inputs = inputs.to(device)\n",
    "            output = model(inputs)\n",
    "            video_features_lbp.append(output.cpu())\n",
    "            video_pred_lbp.append(target)\n",
    "            y_preds = output.argmax(dim=-1).to('cpu').detach().numpy()\n",
    "            if y_pred is not None:\n",
    "                y_pred = np.concatenate([y_pred, y_preds])\n",
    "                target  = np.concatenate([y_test, target])\n",
    "            else:\n",
    "                y_pred = y_preds\n",
    "                y_test = target\n",
    "    f1 = sklearn.metrics.f1_score(y_test, y_preds, average='macro')\n",
    "    acc = sklearn.metrics.accuracy_score(y_test, y_preds)\n",
    "    precision = sklearn.metrics.precision_score(y_test, y_preds, average='macro')\n",
    "    recall = sklearn.metrics.recall_score(y_test, y_preds, average='macro')\n",
    "    print('metrics:', acc, f1, precision, recall)\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics: acc: 0.56 f1: 0.55\n"
     ]
    }
   ],
   "source": [
    "f1 = sklearn.metrics.f1_score(y_test, y_pred, average='macro')\n",
    "acc = sklearn.metrics.accuracy_score(y_test, y_pred)\n",
    "print('metrics:', 'acc:', round(acc,2),'f1:', round(f1,2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LBP признак"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Dataset for pytorch\n",
    "class ImageDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Some Information about MyDataset\"\"\"\n",
    "    def __init__(self,dir_paths = None, labels=None, le = None,  extractor = None):\n",
    "        super(ImageDataset, self).__init__()\n",
    "        self.dir_paths = dir_paths\n",
    "        self.labels = labels\n",
    "        self.extractor = extractor\n",
    "        self.image_features = []\n",
    "        self.set_of_labels = []\n",
    "        for i in self.dir_paths:\n",
    "            output = pd.read_pickle(i)\n",
    "            self.image_features += output['Features'].values[0][0:int(len(output['Features'].values[0])*0.1)]\n",
    "            self.set_of_labels += list(le.transform(output['Labels'].values[0][0:int(len(output['Labels'].values[0])*0.1)]))\n",
    "\n",
    "                \n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        label = self.set_of_labels[index]\n",
    "        features = torch.FloatTensor(np.array([self.image_features[index]]))/255\n",
    "        return features, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract LBP features.Not fast version\n",
    "from tqdm import tqdm\n",
    "from itertools import repeat\n",
    "import concurrent.futures\n",
    "\n",
    "def LBP(img):\n",
    "    arr = np.zeros((len(img),len(img[0])))\n",
    "    img = cv2.copyMakeBorder(img, 1, 1, 1, 1, cv2.BORDER_REFLECT)\n",
    "    img = np.array(img)\n",
    "    for i in range(len(img)-2):\n",
    "        for j in range(len(img[i])-2):\n",
    "            arr[i][j] = (img[i + 1][j + 1] < img[i][j]) * 1 + \\\n",
    "                        (img[i + 1][j + 1] < img[i][j+1]) * 2 + \\\n",
    "                        (img[i + 1][j + 1] < img[i][j+2]) * 4 + \\\n",
    "                        (img[i + 1][j + 1] < img[i+1][j+2]) * 8 + \\\n",
    "                        (img[i + 1][j + 1] < img[i+2][j+2]) * 16 + \\\n",
    "                        (img[i + 1][j + 1] < img[i+2][j+1]) * 32 + \\\n",
    "                        (img[i + 1][j + 1] < img[i+2][j]) * 64 + \\\n",
    "                        (img[i + 1][j + 1] < img[i+1][j]) * 128\n",
    "    return np.uint8(arr)\n",
    "\n",
    "def get_features(file_path):\n",
    "    X_data = []\n",
    "    y_data = []\n",
    "    \n",
    "    for file_path in set_of_classes:\n",
    "        list_of_files = sorted(glob.glob('./CREMA-D/FaceClass/' + file_path + '/*.jpg'))\n",
    "        count = 0\n",
    "        for image_path in list_of_files:\n",
    "            gray = cv2.imread(image_path,cv2.IMREAD_GRAYSCALE)\n",
    "            arr_img = LBP(gray)\n",
    "            display(Image.fromarray(arr_img))\n",
    "            X_data.append(arr_img)\n",
    "            y_data.append(file_path)\n",
    "\n",
    "    return X_data, y_data\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=6) as ex:\n",
    "    SAD_res = ex.submit(get_features, set_of_classes[0])\n",
    "    NEU_res = ex.submit(get_features, set_of_classes[1])\n",
    "    HAP_res = ex.submit(get_features, set_of_classes[2])\n",
    "    FEA_res = ex.submit(get_features, set_of_classes[3])\n",
    "    DIS_res = ex.submit(get_features, set_of_classes[4])\n",
    "    ANG_res = ex.submit(get_features, set_of_classes[5])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SADHAPNEUFEADISANG\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/shemetov@ad.speechpro.com/Desktop/testpy/task.ipynb Cell 24\u001b[0m in \u001b[0;36m<cell line: 77>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bshemetov-ls.ad.speechpro.com/home/shemetov%40ad.speechpro.com/Desktop/testpy/task.ipynb#ch0000023vscode-remote?line=80'>81</a>\u001b[0m     FEA_res \u001b[39m=\u001b[39m ex\u001b[39m.\u001b[39msubmit(get_features, set_of_classes[\u001b[39m3\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bshemetov-ls.ad.speechpro.com/home/shemetov%40ad.speechpro.com/Desktop/testpy/task.ipynb#ch0000023vscode-remote?line=81'>82</a>\u001b[0m     DIS_res \u001b[39m=\u001b[39m ex\u001b[39m.\u001b[39msubmit(get_features, set_of_classes[\u001b[39m4\u001b[39m])\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bshemetov-ls.ad.speechpro.com/home/shemetov%40ad.speechpro.com/Desktop/testpy/task.ipynb#ch0000023vscode-remote?line=82'>83</a>\u001b[0m     ANG_res \u001b[39m=\u001b[39m ex\u001b[39m.\u001b[39msubmit(get_features, set_of_classes[\u001b[39m5\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bshemetov-ls.ad.speechpro.com/home/shemetov%40ad.speechpro.com/Desktop/testpy/task.ipynb#ch0000023vscode-remote?line=84'>85</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mLBP Program is finished\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.8/concurrent/futures/_base.py:644\u001b[0m, in \u001b[0;36mExecutor.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    643\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__exit__\u001b[39m(\u001b[39mself\u001b[39m, exc_type, exc_val, exc_tb):\n\u001b[0;32m--> 644\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshutdown(wait\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    645\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.8/concurrent/futures/process.py:686\u001b[0m, in \u001b[0;36mProcessPoolExecutor.shutdown\u001b[0;34m(self, wait)\u001b[0m\n\u001b[1;32m    684\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_queue_management_thread_wakeup\u001b[39m.\u001b[39mwakeup()\n\u001b[1;32m    685\u001b[0m     \u001b[39mif\u001b[39;00m wait:\n\u001b[0;32m--> 686\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_queue_management_thread\u001b[39m.\u001b[39;49mjoin()\n\u001b[1;32m    687\u001b[0m \u001b[39m# To reduce the risk of opening too many files, remove references to\u001b[39;00m\n\u001b[1;32m    688\u001b[0m \u001b[39m# objects that use file descriptors.\u001b[39;00m\n\u001b[1;32m    689\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_queue_management_thread \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.8/threading.py:1011\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1008\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mcannot join current thread\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1010\u001b[0m \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1011\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_wait_for_tstate_lock()\n\u001b[1;32m   1012\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1013\u001b[0m     \u001b[39m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1014\u001b[0m     \u001b[39m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1015\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[39m=\u001b[39m\u001b[39mmax\u001b[39m(timeout, \u001b[39m0\u001b[39m))\n",
      "File \u001b[0;32m/usr/lib/python3.8/threading.py:1027\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[39mif\u001b[39;00m lock \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:  \u001b[39m# already determined that the C code is done\u001b[39;00m\n\u001b[1;32m   1026\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_stopped\n\u001b[0;32m-> 1027\u001b[0m \u001b[39melif\u001b[39;00m lock\u001b[39m.\u001b[39;49macquire(block, timeout):\n\u001b[1;32m   1028\u001b[0m     lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m   1029\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stop()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#extract LBP features. Fast version with threads\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def get_pixel(img, center, x, y):\n",
    "\tnew_value = 0\n",
    "\ttry:\n",
    "\t\tif img[x][y] >= center:\n",
    "\t\t\tnew_value = 1\n",
    "\texcept:\n",
    "\t\tpass\n",
    "\n",
    "\treturn new_value\n",
    "\n",
    "# Function for calculating LBP\n",
    "def lbp_calculated_pixel(img, x, y):\n",
    "\tcenter = img[x][y]\n",
    "\tval_ar = []\n",
    "\n",
    "\tval_ar.append(get_pixel(img, center, x-1, y-1))\n",
    "\t\n",
    "\tval_ar.append(get_pixel(img, center, x-1, y))\n",
    "\t\n",
    "\tval_ar.append(get_pixel(img, center, x-1, y + 1))\n",
    "\n",
    "\tval_ar.append(get_pixel(img, center, x, y + 1))\n",
    "\t\n",
    "\tval_ar.append(get_pixel(img, center, x + 1, y + 1))\n",
    "\t\n",
    "\tval_ar.append(get_pixel(img, center, x + 1, y))\n",
    "\t\n",
    "\tval_ar.append(get_pixel(img, center, x + 1, y-1))\n",
    "\t\n",
    "\tval_ar.append(get_pixel(img, center, x, y-1))\n",
    "\t\n",
    "\tpower_val = [1, 2, 4, 8, 16, 32, 64, 128]\n",
    "\n",
    "\tval = 0\n",
    "\t\n",
    "\tfor i in range(len(val_ar)):\n",
    "\t\tval += val_ar[i] * power_val[i]\n",
    "\t\t\n",
    "\treturn val\n",
    "\n",
    "\n",
    "def LBP(img):\n",
    "    height, width = 128,128\n",
    "    img_lbp = np.zeros((height, width),\n",
    "\t\t\t\tnp.uint8)\n",
    "    for i in range(0, height):\n",
    "        for j in range(0, width):\n",
    "            img_lbp[i, j] = lbp_calculated_pixel(img, i, j)\n",
    "    return img_lbp\n",
    "\n",
    "def get_features(file_path):\n",
    "    #s = start_time()\n",
    "    X_data = []\n",
    "    y_data = []\n",
    "    count = 0\n",
    "    print(file_path)\n",
    "    list_of_files = sorted(glob.glob('./CREMA-D/FaceClass/' + file_path + '/*.jpg'))\n",
    "\n",
    "    for image_path in list_of_files:\n",
    "        if count % 1000 == 999:\n",
    "            print(file_path,\" - \", count)\n",
    "        gray = cv2.imread(image_path,cv2.IMREAD_GRAYSCALE)\n",
    "        arr_img = LBP(gray)\n",
    "        X_data.append(arr_img)\n",
    "        y_data.append(file_path)\n",
    "        count += 1\n",
    "\n",
    "    return X_data, y_data\n",
    "\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor(max_workers=6) as ex:\n",
    "    SAD_res = ex.submit(get_features, set_of_classes[0])\n",
    "    NEU_res = ex.submit(get_features, set_of_classes[1])\n",
    "    HAP_res = ex.submit(get_features, set_of_classes[2])\n",
    "    FEA_res = ex.submit(get_features, set_of_classes[3])\n",
    "    DIS_res = ex.submit(get_features, set_of_classes[4])\n",
    "    ANG_res = ex.submit(get_features, set_of_classes[5])\n",
    "\n",
    "print(\"LBP Program is finished\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reference to dirs where we want to save our dataframes\n",
    "filepath = '/home/shemetov@ad.speechpro.com/Desktop/testpy/lbp_features_SAD'\n",
    "filepath1 = '/home/shemetov@ad.speechpro.com/Desktop/testpy/lbp_features_NEU'\n",
    "filepath2 = '/home/shemetov@ad.speechpro.com/Desktop/testpy/lbp_features_DIS'\n",
    "filepath3 = '/home/shemetov@ad.speechpro.com/Desktop/testpy/lbp_features_FEA'\n",
    "filepath4 = '/home/shemetov@ad.speechpro.com/Desktop/testpy/lbp_features_ANG'\n",
    "filepath5 = '/home/shemetov@ad.speechpro.com/Desktop/testpy/lbp_features_HAP'\n",
    "filepaths = [filepath,filepath1,filepath2,filepath3,filepath4,filepath5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create DataFrames for each emotion\n",
    "df_SAD = pd.DataFrame(columns = ['Features', 'Labels'])\n",
    "df_NEU = pd.DataFrame(columns = ['Features', 'Labels'])\n",
    "df_DIS = pd.DataFrame(columns = ['Features', 'Labels'])\n",
    "df_FEA = pd.DataFrame(columns = ['Features', 'Labels'])\n",
    "df_ANG = pd.DataFrame(columns = ['Features', 'Labels'])\n",
    "df_HAP = pd.DataFrame(columns = ['Features', 'Labels'])\n",
    "\n",
    "#df.to_pickle(filepath)\n",
    "df_SAD.to_pickle(filepath)\n",
    "df_NEU.to_pickle(filepath1)\n",
    "df_DIS.to_pickle(filepath2)\n",
    "df_FEA.to_pickle(filepath3)\n",
    "df_ANG.to_pickle(filepath4)\n",
    "df_HAP.to_pickle(filepath5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add result of LBP extraction to DataFrames\n",
    "df_SAD = df_SAD.append({'Features':SAD_res.result()[0],'Labels':SAD_res.result()[1]},ignore_index=True)\n",
    "df_NEU = df_NEU.append({'Features':NEU_res.result()[0],'Labels':NEU_res.result()[1]},ignore_index=True)\n",
    "df_DIS = df_DIS.append({'Features':HAP_res.result()[0],'Labels':HAP_res.result()[1]},ignore_index=True)\n",
    "df_FEA = df_FEA.append({'Features':FEA_res.result()[0],'Labels':FEA_res.result()[1]},ignore_index=True)\n",
    "df_ANG = df_ANG.append({'Features':DIS_res.result()[0],'Labels':DIS_res.result()[1]},ignore_index=True)\n",
    "df_HAP = df_HAP.append({'Features':ANG_res.result()[0],'Labels':ANG_res.result()[1]},ignore_index=True)\n",
    "#df.to_pickle(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Формируем датасет из признаков LBP для обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '/home/shemetov@ad.speechpro.com/Desktop/testpy/lbp_features_SAD'\n",
    "filepath1 = '/home/shemetov@ad.speechpro.com/Desktop/testpy/lbp_features_NEU'\n",
    "filepath2 = '/home/shemetov@ad.speechpro.com/Desktop/testpy/lbp_features_DIS'\n",
    "filepath3 = '/home/shemetov@ad.speechpro.com/Desktop/testpy/lbp_features_FEA'\n",
    "filepath4 = '/home/shemetov@ad.speechpro.com/Desktop/testpy/lbp_features_ANG'\n",
    "filepath5 = '/home/shemetov@ad.speechpro.com/Desktop/testpy/lbp_features_HAP'\n",
    "filepaths = [filepath,filepath1,filepath2,filepath3,filepath4,filepath5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57052\n",
      "57051\n",
      "torch.Size([1, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "extractor = None\n",
    "le = sklearn.preprocessing.LabelEncoder()\n",
    "le = le.fit(set_of_classes)\n",
    "\n",
    "data = ImageDataset(dir_paths=filepaths,labels=set_of_classes,le=le,extractor=extractor)\n",
    "print(len(data))\n",
    "print(int(len(data) * 0.8) + int(len(data) * 0.2))\n",
    "print(np.shape(next(iter(data))[0]))\n",
    "train, test = torch.utils.data.random_split(data,[int(len(data) * 0.8)+1,int(len(data) * 0.2)])\n",
    "train_set = torch.utils.data.DataLoader(train, batch_size=4, shuffle=True)\n",
    "test_set = torch.utils.data.DataLoader(test, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, in_channels = 1, out_channels = 128, kernel_size = 5):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels * 2, 5)\n",
    "        self.fc1 = nn.Linear(215296, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 6)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNN().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Первая модель(неиспоьузется)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNet(nn.Module):\n",
    "    def __init__(self, in_channels = 1,out_channels = 128,kernel_size = 5):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.pool1 = nn.MaxPool2d(2,2)\n",
    "        self.drop1 = nn.Dropout(0.5)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3)\n",
    "        self.pool2 = nn.MaxPool2d(2,2)\n",
    "        self.drop2 = nn.Dropout(0.5)\n",
    "        self.conv3 = nn.Conv2d(out_channels, 2 * out_channels, 3)\n",
    "        self.bn3 = nn.BatchNorm2d(2 * out_channels)\n",
    "        self.drop3 = nn.Dropout(0.2)\n",
    "        self.pool3 = nn.MaxPool2d(2,2)\n",
    "        self.conv4 = nn.Conv2d(2 * out_channels, 2 * out_channels, 3)\n",
    "        self.bn4 = nn.BatchNorm2d(2 * out_channels)\n",
    "        self.drop4 = nn.Dropout(0.2)\n",
    "        self.pool4 = nn.MaxPool2d(2,2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(50176, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 6)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.pool1(self.conv1(x)))\n",
    "        x = F.relu(self.pool2(self.conv2(x)))\n",
    "        x = F.relu(self.pool3(self.conv3(x)))\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        return F.log_softmax(x,dim=1)  \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "def conv_block(in_channels, out_channels, pool=False):\n",
    "    layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1), \n",
    "              nn.BatchNorm2d(out_channels), \n",
    "              nn.ELU(inplace=True)]\n",
    "    if pool: layers.append(nn.MaxPool2d(2))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = conv_block(in_channels, 128)\n",
    "        self.conv2 = conv_block(128, 128, pool=True)\n",
    "        self.res1 = nn.Sequential(conv_block(128, 128), conv_block(128, 128))\n",
    "        self.drop1 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.conv3 = conv_block(128, 256)\n",
    "        self.conv4 = conv_block(256, 256, pool=True)\n",
    "        self.res2 = nn.Sequential(conv_block(256, 256), conv_block(256, 256))\n",
    "        self.drop2 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.conv5 = conv_block(256, 512)\n",
    "        self.conv6 = conv_block(512, 512, pool=True)\n",
    "        self.res3 = nn.Sequential(conv_block(512, 512), conv_block(512, 512))\n",
    "        self.drop3 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.classifier = nn.Sequential(nn.MaxPool2d(6), \n",
    "                                        nn.Flatten(),\n",
    "                                        nn.Linear(2048, num_classes))\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        out = self.conv1(xb)\n",
    "        out = self.conv2(out)\n",
    "        out = self.res1(out) + out\n",
    "        out = self.drop1(out)\n",
    "        \n",
    "        out = self.conv3(out)\n",
    "        out = self.conv4(out)\n",
    "        out = self.res2(out) + out\n",
    "        out = self.drop2(out)\n",
    "        \n",
    "        out = self.conv5(out)\n",
    "        out = self.conv6(out)\n",
    "        out = self.res3(out) + out\n",
    "        out = self.drop3(out)\n",
    "        \n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model = ResNet(1,6).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.008, weight_decay= 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training model\n",
    "\n",
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in tqdm(enumerate(train_set, 0)):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 1000:.3f}')\n",
    "            running_loss = 0.0\n",
    "    #print('loss:', sum(running_loss)/len(running_loss))\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model for future\n",
    "torch.save(model, '/home/shemetov@ad.speechpro.com/Desktop/testpy/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('/home/shemetov@ad.speechpro.com/Desktop/testpy/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 357/357 [00:06<00:00, 52.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics: acc: 0.9757230499561788 0.9753961959838612 0.9756138376804229 0.9753374731341701\n"
     ]
    }
   ],
   "source": [
    "#Validation\n",
    "\n",
    "y_preds = None\n",
    "targets = None\n",
    "video_features_lbp = []\n",
    "video_pred_lbp = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "  for inputs,target in tqdm(test_set):\n",
    "    video_pred_lbp.append(target)    \n",
    "    inputs = inputs.to(device)\n",
    "    output = model(inputs)\n",
    "    video_features_lbp.append(output.cpu())\n",
    "   #print(np.shape(output))\n",
    "    y_pred = output.argmax(dim=-1).to('cpu').detach().numpy()\n",
    "    #print(y_pred,target)\n",
    "    if y_preds is not None:\n",
    "            y_preds = np.concatenate([y_preds, y_pred])\n",
    "            targets  = np.concatenate([targets, target])\n",
    "    else:\n",
    "            y_preds = y_pred\n",
    "            targets = target\n",
    "f1 = sklearn.metrics.f1_score(targets, y_preds, average='macro')\n",
    "acc = sklearn.metrics.accuracy_score(targets, y_preds)\n",
    "precision = sklearn.metrics.precision_score(targets, y_preds, average='macro')\n",
    "recall = sklearn.metrics.recall_score(targets, y_preds, average='macro')\n",
    "print('metrics:', 'acc:', acc, f1, precision, recall)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------- Аудио сектор ----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mel(spec):\n",
    "    spec = 1125 * np.log(1 + spec/700)\n",
    "    return spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Dataset with MFCC features for pytorch\n",
    "\n",
    "class WavDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Some Information about MyDataset\"\"\"\n",
    "    def __init__(self,dir_path = None, labels=None, le = None,  extractor = None):\n",
    "        super(WavDataset, self).__init__()\n",
    "        self.dir_path = dir_path\n",
    "        self.labels = labels\n",
    "        self.extractor = extractor\n",
    "        self.pad_size = 0\n",
    "        self.wav_files = []\n",
    "        self.set_of_labels = []\n",
    "        count = 0\n",
    "        if self.labels is not None:\n",
    "            for label in self.labels:\n",
    "                wav_paths = glob.glob(dir_path + label + '/*.wav')\n",
    "                len_wav_files = len(wav_paths)\n",
    "                label = list(le.transform([label]))\n",
    "                self.set_of_labels += label * len_wav_files\n",
    "                self.wav_files += wav_paths\n",
    "                for i in wav_paths:\n",
    "                    wav = torchaudio.load(i)[0][0]\n",
    "                    self.pad_size += len(wav)\n",
    "                    count += 1\n",
    "            self.pad_size = self.pad_size // count\n",
    "\n",
    "                \n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        start_time = time.time()\n",
    "        wav, sr =  torchaudio.load(self.wav_files[index])\n",
    "        label = self.set_of_labels[index]\n",
    "        if self.pad_size:\n",
    "            p = (self.pad_size - len(wav.flatten())) // 2 + 1\n",
    "            if p>0:\n",
    "                wav = torch.nn.functional.pad(wav, (p, p), value=0.0)\n",
    "            wav = wav[:, :self.pad_size]\n",
    "        if self.extractor is not None:\n",
    "            features = extractor(wav)\n",
    "        #print(\"debug __get_item__\",\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "        return features, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.wav_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting for MFCC\n",
    "\n",
    "n_fft = 2048\n",
    "win_length = None\n",
    "hop_length = 512\n",
    "n_mels = 256\n",
    "n_mfcc = 256\n",
    "\n",
    "mfcc_transform = T.MFCC(\n",
    "    n_mfcc=n_mfcc,\n",
    "    melkwargs={\n",
    "        \"n_fft\": n_fft,\n",
    "        \"n_mels\": n_mels,\n",
    "        \"hop_length\": hop_length,\n",
    "        \"mel_scale\": \"htk\",\n",
    "    },\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7442\n",
      "7441\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "#Traind and val sets\n",
    "\n",
    "extractor = mfcc_transform\n",
    "le = sklearn.preprocessing.LabelEncoder()\n",
    "le = le.fit(set_of_classes)\n",
    "dir_path = '/home/shemetov@ad.speechpro.com/Desktop/testpy/CREMA-D/AudioClass/'\n",
    "\n",
    "data_wav = WavDataset(dir_path=dir_path,labels=set_of_classes,le=le,extractor=extractor)\n",
    "print(len(data_wav))\n",
    "print(int(len(data_wav) * 0.8) + int(len(data_wav) * 0.2))\n",
    "\n",
    "print(next(iter(data_wav))[1])\n",
    "train_wav, test_wav = torch.utils.data.random_split(data_wav,[int(len(data_wav) * 0.8)+1,int(len(data_wav) * 0.2)])\n",
    "train_set_wav = torch.utils.data.DataLoader(train_wav, batch_size=32, shuffle=True)\n",
    "test_set_wav = torch.utils.data.DataLoader(test_wav, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNet(nn.Module):\n",
    "    def __init__(self, in_channels = 1, out_channels = 128, kernel_size = 5):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels * 2, 5)\n",
    "        self.fc1 = nn.Linear(265472, 128)\n",
    "        self.fc2 = nn.Linear(128, 84)\n",
    "        self.fc3 = nn.Linear(84, 6)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNet(nn.Module):\n",
    "    def __init__(self, in_channels = 1,out_channels = 32,kernel_size = 5):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.pool1 = nn.MaxPool2d(2,2)\n",
    "        self.drop1 = nn.Dropout(0.2)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.pool2 = nn.MaxPool2d(2,2)\n",
    "        self.drop2 = nn.Dropout(0.2)\n",
    "        self.conv3 = nn.Conv2d(out_channels, 2 * out_channels, 3)\n",
    "        self.bn3 = nn.BatchNorm2d(2 * out_channels)\n",
    "        self.drop3 = nn.Dropout(0.2)\n",
    "        self.pool3 = nn.MaxPool2d(2,2)\n",
    "        self.conv4 = nn.Conv2d(2 * out_channels, 2 * out_channels, 3)\n",
    "        self.bn4 = nn.BatchNorm2d(2 * out_channels)\n",
    "        self.drop4 = nn.Dropout(0.2)\n",
    "        self.pool4 = nn.MaxPool2d(2,2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(2688, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 6)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.pool1(self.bn1(self.conv1(x))))\n",
    "        x = self.drop1(x)\n",
    "        x = F.relu(self.pool2(self.bn2(self.conv2(x))))\n",
    "        x = self.drop2(x)\n",
    "        x = F.relu(self.pool3(self.bn3(self.conv3(x))))\n",
    "        x = self.drop3(x)\n",
    "        x = F.relu(self.pool4(self.bn4(self.conv4(x))))\n",
    "        x = self.drop4(x)\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        return F.log_softmax(x,dim=1)  \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for epoch in range(15):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in tqdm(enumerate(train_set_wav, 0)):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 100:.3f}')\n",
    "            running_loss = 0.0\n",
    "    #print('loss:', sum(running_loss)/len(running_loss))\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, '/home/shemetov@ad.speechpro.com/Desktop/testpy/modelCNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('/home/shemetov@ad.speechpro.com/Desktop/testpy/modelCNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:02<00:00, 17.71it/s]\n"
     ]
    }
   ],
   "source": [
    "y_pred = []\n",
    "y_test = []\n",
    "audio_features_mfcc = []\n",
    "audio_pred_mfcc = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "  for inputs,target in tqdm(test_set_wav):\n",
    "    inputs = inputs.to(device)\n",
    "    audio_pred_mfcc.append(target)\n",
    "    y_test.extend(target.numpy())\n",
    "    output = model(inputs)\n",
    "    audio_features_mfcc.append(output.cpu())\n",
    "    #print(np.shape(output))\n",
    "    for idx, i in enumerate(output):\n",
    "      y_pred.append(torch.argmax(i).item())\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.77      0.67       235\n",
      "           1       0.45      0.56      0.50       245\n",
      "           2       0.54      0.42      0.47       241\n",
      "           3       0.59      0.57      0.58       256\n",
      "           4       0.60      0.53      0.56       250\n",
      "           5       0.58      0.51      0.54       261\n",
      "\n",
      "    accuracy                           0.56      1488\n",
      "   macro avg       0.56      0.56      0.55      1488\n",
      "weighted avg       0.56      0.56      0.55      1488\n",
      "\n",
      "metrics: acc: 0.5571236559139785 f1: 0.5545697742847703 prec: 0.5598225099889815 recall: 0.558768491288896\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))\n",
    "f1 = sklearn.metrics.f1_score(y_test, y_pred, average='macro')\n",
    "acc = sklearn.metrics.accuracy_score(y_test, y_pred)\n",
    "precision = sklearn.metrics.precision_score(y_test, y_pred, average='macro')\n",
    "recall = sklearn.metrics.recall_score(y_test, y_pred, average='macro')\n",
    "print('metrics:', 'acc:',acc,'f1:',f1,'prec:',precision,'recall:',recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Объединение видеомодальности и аудиомодальности. Начало раздела"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X = video_features_lbp + audio_features_mfcc\n",
    "new_y = video_pred_lbp + audio_pred_mfcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_XX = []\n",
    "for i in range(len(new_X)):\n",
    "    for j in range(len(new_X[i])):\n",
    "        new_XX.append(new_X[i][j].tolist())\n",
    "        \n",
    "new_yy = []\n",
    "for i in range(len(new_y)):\n",
    "    for j in range(len(new_y[i])):\n",
    "        new_yy.append(new_y[i][j].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(249563, 6)\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(new_XX))\n",
    "print(new_yy[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('svc', SVC(gamma='auto'))])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "clf.fit(new_XX,new_yy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 3, ..., 1, 5, 4])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_preds = clf.predict(new_XX[200000:])\n",
    "y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics: acc: 0.5093920868389726 0.37020686433960853 0.5492275895637287 0.40596049961328795\n"
     ]
    }
   ],
   "source": [
    "f1 = sklearn.metrics.f1_score(new_yy[200000:], y_preds, average='macro')\n",
    "acc = sklearn.metrics.accuracy_score(new_yy[200000:], y_preds)\n",
    "precision = sklearn.metrics.precision_score(new_yy[200000:], y_preds, average='macro')\n",
    "recall = sklearn.metrics.recall_score(new_yy[200000:], y_preds, average='macro')\n",
    "print('metrics:', 'acc:', acc, f1, precision, recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Объединение видеомодальности и аудиомодальности. Конец раздела"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Dataset with OpenSmile features\n",
    "\n",
    "class WavSmileDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Some Information about MyDataset\"\"\"\n",
    "    def __init__(self,dir_path = None, smile = None, labels=None, le = None):\n",
    "        super(WavSmileDataset, self).__init__()\n",
    "        self.dir_path = dir_path\n",
    "        self.labels = labels\n",
    "        self.smile = smile\n",
    "        self.features = []\n",
    "        self.set_of_name_of_features = smile.feature_names\n",
    "        self.wav_files = []\n",
    "        self.features = []\n",
    "        self.set_of_labels = []\n",
    "        if self.labels is not None:\n",
    "            for label in self.labels:\n",
    "                wav_paths = glob.glob(dir_path + label + '/*.wav')\n",
    "                len_wav_files = len(wav_paths)\n",
    "                label = list(le.transform([label]))\n",
    "                self.set_of_labels += label * len_wav_files\n",
    "                self.wav_files += wav_paths\n",
    "        for i in tqdm(self.wav_files):\n",
    "            self.features.append(smile.process_file(i).values[0])\n",
    "\n",
    "                    \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        label = self.set_of_labels[index]\n",
    "        feature = self.features[index]\n",
    "        return feature, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.wav_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7442/7442 [06:50<00:00, 18.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7442\n",
      "7441\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "le = sklearn.preprocessing.LabelEncoder()\n",
    "le = le.fit(set_of_classes)\n",
    "dir_path = '/home/shemetov@ad.speechpro.com/Desktop/testpy/CREMA-D/AudioClass/'\n",
    "\n",
    "smile = opensmile.Smile(\n",
    "    feature_set=opensmile.FeatureSet.ComParE_2016,\n",
    "    feature_level=opensmile.FeatureLevel.Functionals,\n",
    ")\n",
    "\n",
    "data_smile = WavSmileDataset(dir_path=dir_path,labels=set_of_classes,le=le,smile=smile)\n",
    "print(len(data_smile))\n",
    "print(int(len(data_smile) * 0.8) + int(len(data_smile) * 0.2))\n",
    "\n",
    "print(next(iter(data_smile))[1])\n",
    "train_smile, test_smile = torch.utils.data.random_split(data_smile,[int(len(data_smile) * 0.8)+1,int(len(data_smile) * 0.2)])\n",
    "train_set_smile = torch.utils.data.DataLoader(train_smile, batch_size=32, shuffle=True)\n",
    "test_set_smile = torch.utils.data.DataLoader(test_smile, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "X_train_set_smile = sc.fit_transform(next(iter(train_set_smile))[0])\n",
    "print(X_train_set_smile)\n",
    "print(next(iter(train_set_smile))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(6373,128)\n",
    "        self.drop1 = nn.Dropout(0.2)\n",
    "        self.fc2 = nn.Linear(128,128)\n",
    "        self.drop2 = nn.Dropout(0.2)\n",
    "        self.fc3 = nn.Linear(128,84)\n",
    "        self.drop3 = nn.Dropout(0.2)\n",
    "        self.fc4 = nn.Linear(84,6)\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.drop1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.drop2(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.drop3(x)\n",
    "        x = self.fc4(x)\n",
    "        return F.log_softmax(x,dim=1)  \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "modelFC = FC().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "sc = StandardScaler()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(modelFC.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for epoch in range(50):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in tqdm(enumerate(train_set_smile, 0)):\n",
    "        \n",
    "        inputs, labels = data\n",
    "        inputs = torch.FloatTensor(sc.fit_transform(inputs))\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = modelFC(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 ==999:\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 1000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "y_test = []\n",
    "audio_features_mfcc = []\n",
    "audio_pred_mfcc = []\n",
    "modelFC.eval()\n",
    "with torch.no_grad():\n",
    "  for inputs,target in test_set_smile:\n",
    "    audio_pred_mfcc.append(target)\n",
    "    inputs = torch.FloatTensor(sc.fit_transform(inputs))\n",
    "    inputs = inputs.to(device)\n",
    "    y_test.extend(target.numpy())\n",
    "    output = modelFC(inputs)\n",
    "    audio_features_mfcc.append(output.cpu())\n",
    "    for idx, i in enumerate(output):\n",
    "      y_pred.append(torch.argmax(i).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.78      0.74       250\n",
      "           1       0.62      0.48      0.54       254\n",
      "           2       0.49      0.54      0.52       243\n",
      "           3       0.61      0.58      0.59       245\n",
      "           4       0.66      0.63      0.64       238\n",
      "           5       0.53      0.59      0.56       258\n",
      "\n",
      "    accuracy                           0.60      1488\n",
      "   macro avg       0.60      0.60      0.60      1488\n",
      "weighted avg       0.60      0.60      0.60      1488\n",
      "\n",
      "metrics: acc: 0.6008064516129032 f1: 0.6001046425847846 prec: 0.6036434994347634 recall: 0.6010412342971783\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "f1 = sklearn.metrics.f1_score(y_test, y_pred, average='macro')\n",
    "acc = sklearn.metrics.accuracy_score(y_test, y_pred)\n",
    "precision = sklearn.metrics.precision_score(y_test, y_pred, average='macro')\n",
    "recall = sklearn.metrics.recall_score(y_test, y_pred, average='macro')\n",
    "print('metrics:', 'acc:',acc,'f1:',f1,'prec:',precision,'recall:',recall)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
